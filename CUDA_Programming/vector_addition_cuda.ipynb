{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ciJvN4gLttG",
        "outputId": "14ed5c2b-19ee-4d94-cc9b-b3b300eba121"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing vecadd.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile vecadd.cu\n",
        "\n",
        "//Vector Addition on GPU using CUDA\n",
        "\n",
        "#include <cstdlib>\n",
        "#include <cassert>\n",
        "#include <iostream>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "__global__\n",
        "void vecAdd(int *a, int *b, int *c, int N){\n",
        "  int thread = blockIdx.x*blockDim.x + threadIdx.x;\n",
        "\n",
        "  if(thread < N){\n",
        "    c[thread] = a[thread] + b[thread];\n",
        "  }\n",
        "}\n",
        "\n",
        "void verifyAdd(int *a, int *b, int *c, int N){\n",
        "  for(int i = 0; i < N; i++){\n",
        "    assert(c[i] == a[i] + b[i]);\n",
        "  }\n",
        "}\n",
        "\n",
        "void initMatrix(int* a, int N){\n",
        "  for(int i = 0; i < N; i++){\n",
        "    a[i] = rand() % 100; // number from 0-99\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "int main(){\n",
        "  int N = 1 << 10;\n",
        "  size_t sz = N*sizeof(int);\n",
        "\n",
        "  //allocating and initializing vectors\n",
        "  int *h_a, *h_b, *h_c;\n",
        "  h_a = (int*)malloc(sz);\n",
        "  h_b = (int*)malloc(sz);\n",
        "  h_c = (int*)malloc(sz);\n",
        "\n",
        "  int *d_a, *d_b, *d_c;\n",
        "  cudaMalloc(&d_a, sz);\n",
        "  cudaMalloc(&d_b, sz);\n",
        "  cudaMalloc(&d_c, sz);\n",
        "\n",
        "\n",
        "  initMatrix(h_a, N);\n",
        "  initMatrix(h_b, N);\n",
        "  \n",
        "  cudaMemcpy(d_a, h_a, sz, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(d_b, h_b, sz, cudaMemcpyHostToDevice);\n",
        "\n",
        "  int threads = 256;\n",
        "  int blocks = (N + threads - 1) / threads;\n",
        "  dim3 grid_size(blocks, 1, 1);\n",
        "  dim3 block_size(threads, 1, 1);\n",
        "\n",
        "  //Launching Kernel\n",
        "  vecAdd<<<grid_size, block_size>>>(d_a, d_b, d_c, N);\n",
        "  cudaDeviceSynchronize();\n",
        "  cudaMemcpy(h_c, d_c, sz, cudaMemcpyDeviceToHost);\n",
        "\n",
        "  //Verifying the result\n",
        "  verifyAdd(h_a, h_b, h_c, N);\n",
        "\n",
        "  return 0;\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -o vecadd vecadd.cu"
      ],
      "metadata": {
        "id": "LpQfawpdL3bY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./vecadd"
      ],
      "metadata": {
        "id": "wX7nz9ISL-TG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof --print-gpu-trace ./vecadd"
      ],
      "metadata": {
        "id": "UKRl7i-INZ2g",
        "outputId": "7d3d9a50-6655-4d1f-b000-61e0f69c2c1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==389== NVPROF is profiling process 389, command: ./vecadd\n",
            "==389== Profiling application: ./vecadd\n",
            "==389== Profiling result:\n",
            "   Start  Duration            Grid Size      Block Size     Regs*    SSMem*    DSMem*      Size  Throughput  SrcMemType  DstMemType           Device   Context    Stream  Name\n",
            "408.25ms  2.1120us                    -               -         -         -         -  4.0000KB  1.8062GB/s    Pageable      Device     Tesla T4 (0)         1         7  [CUDA memcpy HtoD]\n",
            "408.27ms  1.6960us                    -               -         -         -         -  4.0000KB  2.2492GB/s    Pageable      Device     Tesla T4 (0)         1         7  [CUDA memcpy HtoD]\n",
            "408.30ms  4.9600us              (4 1 1)       (256 1 1)        16        0B        0B         -           -           -           -     Tesla T4 (0)         1         7  vecAdd(int*, int*, int*, int) [117]\n",
            "408.33ms  2.2710us                    -               -         -         -         -  4.0000KB  1.6797GB/s      Device    Pageable     Tesla T4 (0)         1         7  [CUDA memcpy DtoH]\n",
            "\n",
            "Regs: Number of registers used per CUDA thread. This number includes registers used internally by the CUDA driver and/or tools and can be more than what the compiler shows.\n",
            "SSMem: Static shared memory allocated per CUDA block.\n",
            "DSMem: Dynamic shared memory allocated per CUDA block.\n",
            "SrcMemType: The type of source memory accessed by memory operation/copy\n",
            "DstMemType: The type of destination memory accessed by memory operation/copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eTXEUqF0NbcV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}